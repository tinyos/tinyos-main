==============================================================================
The Net2 Protocol Benchmark
==============================================================================

:TEP: 140
:Group: Network Working Group 
:Type: Informational
:Status: Draft
:TinyOS-Version: > 2.1
:Author: Thanh Dang, Kaisen Lin, Chieh-Jan Mike Liang, and Omprakash Gnawali

:Draft-Created: 12-Jul-2010
:Draft-Version: $Revision: 1.1 $
:Draft-Modified: $Date: 2010-07-12 22:40:39 $
:Draft-Discuss: TinyOS Developer List <tinyos-devel at mail.millennium.berkeley.edu>

.. Note::

   This memo documents a part of TinyOS for the TinyOS Community, and
   requests discussion and suggestions for improvements.  Distribution
   of this memo is unlimited. This memo is in full compliance with
   TEP 1.

Abstract
==============================================================================

The memo describes the metrics, test scenarios, test test
applications, and analysis tools for testing collection and
dissemination protocols in TinyOS 2.x. Our goal is to design
benchmarks for network protocols.


1. Introduction
==============================================================================

TinyOS 2.x comes with a number of collection and dissemination
protocols. Collection is a network-layer best-effort protocol that
forms routes from all the nodes in a network to the collection roots
or sinks [1]_. CTP [2]_ and MultihopLQI collection protocols are available
in TinyOS 2.x. Dissemination is a network-layer reliable protocol that
allows dissemination of key-value pairs to the entire network
[3]_. Drip, DIP, and DHV dissemination protocols are available in
TinyOS 2.x. Although a large number of mature protocols are available,
we lack standard performance tests to evaluate these protocols.

In this document, we describe benchmarks that allow us to
systematically compare the performance of these protocols. Benchmarks
also enable us to compare improvements to these protocols using a
standard set of test cases, scenarios, and tools. We hope the
community will adopt these benchmarks so that the results from
different research projects can be more directly compared.


2. The Net2 Protocol Benchmark Overview
==============================================================================

There are two components in this benchmark. First, a description of a
series of tests to run to evaluate the protocols. We describe the
topologies in which to test the protocols, the protocol use scenarios,
and protocol parameters to vary across the experiments. The second
component of the net2 protocol benchmark is the metrics that describe
the performance of the protocols when we run them on the series of
scenarios included in the benchmark.


3. Topologies
==============================================================================

To make sure the experiments can be repeatable, the network topologies 
MUST remain the same when the experiments are repeated for different 
protocols. Due to the dynamic nature of testbeds, it is difficult to
repeat an experiment with an exact topology on testbeds. We therefore focus 
on only specifying topologies for simulations in this benchmark.


Topologies used in simulations can be created by either duplicating topologies 
of testbeds or by artificial generation. For testbed topologies, there are 
available topologies from  Motelab, Mirage,...?, which can be accessible at
urls..?. For artificial topologies, experiments SHOULD use the following 
topologies.

a) Single-hop star topology
b) Single-hop clique topology
c) Multi-hop chain tolology
d) Multi-hop grid topology
e) Multi-hop random topology
                                              
               ____                  __ __
 \ | /        /\  /\                |  |  |
__\|/__      /__\/__\     _ _ _ _   |__|__|          
  /|\        \  /\  /               |  |  | 
 / | \        \/__\/                |__|__| 
  a)            b)          c)        d)                  e)

The network size (total number of nodes) and the network density (average 
number of neighbors) can be varied but should remain the same for the same 
set of experiments.

The network dynamic, which is defined by the rate at which nodes 
(eg. unreliable or mobile nodes) join and leave a network MUST also remain 
the same for the same set of experiments.


4. Protocol Use Scenarios
==============================================================================

The following scenarios SHOULD be considered in evaluating dissemination 
protocols:

Scenario 1: A node disseminates one or more new items to a stable network.
This scenario occurs in practice when a node or a basestation reprograms a 
network or disseminates events to all nodes in the network.

Scenario 2: A node joins an updated network. This scenario occurs in
practice when one or more items have been disseminated to the network. 
However, there are new nodes (eg. mobile nodes moving into the network 
region, or intermitent nodes that were off during the dissemination) join 
the network.

Scenario 3: Multiple nodes joining and leaving a network at a specified rate.
This scenario occurs in practice where mobile nodes moving in and out a
region under dissemination.

Scenario 4: Multiple items with different versions are disseminated to
the network from different nodes. This scenario occurs in practice where 
multiple users accessing a shared sensing network and commanding the
network to perform different tasks at the same time. 


The following scenarios SHOULD be considered in evaluating collection 
protocols:

...[for Mike]

5. Protocol Parameters
==============================================================================

There are a number of parameters affecting the performance of collection
and dissemination protocols. Protocol performance MUST be described together 
with the parameters' values. The parameters are organized in order based on 
the network stack. 


[TODO: put all in one table]
+------------------------------------------------------------------------------+
|  
|
+------------------------------------------------------------------------------+
|  Application 
|
|
|
|
|
|
|
|
+-----------------------------------------------------------------------------+





Link layer: 
   + Link quality (TOSSIM only) 
   + LPL/no LPL

Network layer 
   + Trickle Suppression constant (if protocols is trickle-based)

Application layer
   + Item size (within a TOS message)
   + Total number of items
   + Total number of new items



The following are the  meaningful values for each parater.

 + (20) Link layer  
	- (10) link quality
        - (2)  LPL/no LPL

 + (3) Network layer
	- (3) suppression constant : 1,2,3

 + (450) Application layer
	- (2) Item size                  : 2, 10 (Bytes)
  - (15) Total number of items     : 8, 16,..., 128
  - (15) Total number of new items : 8, 16,..., 128
 


 + (3) Network desity
	- (3) Network density : sparse, medium, dense
 
 + (5) Network dynamic
	- joining/leaving rate : 1%, 5%, 10%, 15%, 20%

6. Running the Benchmark
==============================================================================

The complete benchmark requires considering all possible parameter settings for
all network topologies. The total number of test cases is 11,340,000.
 
To remove randomness factors, each test case SHOULD be repeated 10 times. 
Hence, the total number of experiments are 113,400,000.

Depending on which metrics are being compared, experimental design techniques 
SHOULD be used to reduce the number of experiments. 


7. Metrics
==============================================================================

The following metrics SHOULD be considered for dissemination protocol 
comparisions.
 
Total number of transmitted messages: The messages are counted from the time
new items are disseminated to the time the network is completely updated (or
stable in the dynamic network case). 
 
Updating latency for a node: The latency is calculated from the time new items 
are disseminated or the time the node is active (whichever later) to the time 
the node is updated.

Update latency for the whole network: The latency is calculated from the time 
new items are disseminated to the time the network is completely updated 
(or stable in dynamic network case).

Fraction of the network that is updated: In the dynamic network case, it is 
impossible to achieve 100% of the network updated because there are always new 
nodes joining the network and nodes leaving the network. It is suitable to 
consider the fraction of the networki (number of updated nodes/total number 
of nodes) that is updated. This fraction is measured when it is stable.

ROM and RAM usage: 

 
[Energy consumption: remove?]

8. Result Analysis Tools
==============================================================================

Analysis scripts (eg. python, shell, matlab) are to be developed and
released (in tinyos-2.x/support?)


9. Authors
====================================================================

| Thanh Dang
| 135 FAB, 1900 SW 4th Ave
| Portland State University
| Portland, OR 97201
|
| email - dangtx@pdx.edu
|
| Kaisen Lin
| UCSD
| email - kaisenl@cs.ucsd.edu
|
| Chieh-Jan Mike Liang
| 213 NEB, 3400 N Charles St
| Johns Hopkins University
| Baltimore, MD 21211
|
| email - cliang4@cs.jhu.edu
|
| Omprakash Gnawali
| S255 Clark Center, 318 Campus Drive
| Stanford University
| Stanford, CA  94305
|
| phone - +1 650 725 6086
| email - gnawali@cs.stanford.edu


10. Citations
====================================================================

.. [1] TEP 119: Collection

.. [2] TEP 123: The Collection Tree Protocol (CTP) 

.. [2] TEP 118: Dissemination of Small Values
